{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61925d00-502a-43b5-b412-a7287eb0ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a \n",
    "        KNN classifier or regressor?\n",
    "\n",
    "    Ans: The main difference between Euclidean distance and Manhattan distance is how they measure the distance between two points. Euclidean distance calculates the \n",
    "         straight-line distance, while Manhattan distance calculates the sum of absolute differences along each dimension. This difference can affect the performance of a \n",
    "         KNN classifier or regressor by altering the influence of each feature on the distance calculation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37504a-81a1-4b97-a139-1c51b1c34b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "\n",
    "    Ans: To choose the optimal value of k for a KNN classifier or regressor, techniques like cross-validation, grid search, or elbow method can be used. Cross-validation helps \n",
    "         evaluate k's performance on different data subsets. Grid search systematically explores a range of k values. The elbow method looks for the point of diminishing returns \n",
    "         in accuracy or error with increasing k.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfadab4-bcce-4c4c-8e87-ee18592dd4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "\n",
    "    Ans: The choice of distance metric in KNN affects the performance by determining how distances between points are measured. Euclidean distance works well with continuous, \n",
    "         numeric features, while Manhattan distance is suitable for high-dimensional data or when features have different scales. The choice depends on the data characteristics \n",
    "         and problem requirements.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b9cfe-6d26-4b82-92d3-bd4fa151f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these \n",
    "        hyperparameters to improve model performance?\n",
    "\n",
    "    Ans: n_neighbors: Determines the number of neighbors to consider. Higher values increase model complexity but may lead to overfitting.\n",
    "         weights: Determines the weight assigned to each neighbor during prediction. Options include 'uniform' (equal weight) and 'distance' (weight by inverse of distance).\n",
    "         p: Determines the power parameter for Minkowski distance metric. Default is 2 for Euclidean distance and 1 for Manhattan distance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7b59e-bf6a-4c47-8fd7-bc0b482fd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "\n",
    "    Ans: The size of the training set in KNN affects performance. Too small may result in high variance and overfitting, while too large can increase computation time. \n",
    "         Techniques like learning curves can help analyze the impact of different training set sizes, while techniques like resampling or data augmentation can optimize \n",
    "         training set size.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f82bd-a88d-4a39-aaae-47c2fcde7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
    "\n",
    "    Ans: Some potential drawbacks of KNN include: sensitivity to feature scaling, computationally expensive for large datasets, and the need for optimal choice of hyperparameters.\n",
    "         To overcome these drawbacks, feature scaling techniques can be applied, dimensionality reduction methods can be used, and hyperparameters can be tuned using techniques \n",
    "         like cross-validation or grid search.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
